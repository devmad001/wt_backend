import os
import sys
import codecs
import json
import re

LOCAL_PATH = os.path.abspath(os.path.dirname(__file__))+"/"
sys.path.insert(0,LOCAL_PATH)
sys.path.insert(0,LOCAL_PATH+"../")

from a_agent.iagent import Smart_Agent

## Main pipeline (base extraction)
#from w_pipeline.run_pipeline import interface_run_pipeline
#from kb_ai.interface_kbai import interface_run_all_trigger_questions_KB_AI
#from w_ui.sim_user import interface_dump_excel

from kb_ask.kb_ask import interface_kb_ask

from get_logger import setup_logging
from w_mindstate.mindstate import Mindstate
from w_ui.sim_user import interface_get_job_status

logging=setup_logging()


#0v2# JC  Sep 26, 2023  Tie in to greater mindstate (for web + backend)
#0v1# JC  Sep 21, 2023  Setup

"""
    ROUTE TO CORE API SERVICES
    - for now just chatbot support endpoint
    - > see run_scale_worker.py for older version
    - handle config options etc.
"""

config_file=LOCAL_PATH+'../'+'w_config_dashboard.json'
def load_config():
    global config_file
    with open(config_file, 'r') as fp:
        CONFIG = json.load(fp)
    return CONFIG

CONFIG=load_config()
CHATBOT_ONLINE=CONFIG['CHATBOT_ONLINE']

#> imported by wt_flask_handler
class Bot_Interface:
    def __init__(self):
        self.Mind=Mindstate()
        self.Agent=None #Not sure about fully here but saves on connects
        self.case_id=None
        self.sesh_id=None  #generated by backend [ ] setup to allow front pass
        return
    
    def set_case_id(self,case_id):
        self.case_id=case_id
        ## Start or load session
        sesh,sesh_id,is_created=self.Mind.start_or_load_session(case_id=case_id)
        self.sesh_id=sesh_id
        return

    def handle_bot_query(self,query,params={},allow_cached=True):
        # allow_cached::  allow cached response??  not applied yet since pre-written queries are stable(not during auto_50q.py cause want latest processed attempt)

        response='blaa bot'
        answer_dict={}

        case_id=self.case_id #'case_schoolkids'
        
        self.Mind.MIND_store_last_question(query,self.sesh_id)
        
        ## Check status of case
        is_running,has_job_ever_finished,running_msg=self.get_job_status(case_id)

        ### MAIN ROUTER ENTRYPOINT
        #- online? running? ready?

        if CHATBOT_ONLINE:
            
            if not is_running:
                answer,answer_dict,self.Agent=interface_kb_ask(query, case_id=case_id,params=params, Agent=self.Agent,allow_cached=allow_cached)
            elif is_running and has_job_ever_finished:
                answer,answer_dict,self.Agent=interface_kb_ask(query, case_id=case_id,params=params, Agent=self.Agent,allow_cached=allow_cached)
                answer+=' *Case knowledge is being refreshed'
            else:
                ## Is running for the first time
                answer_dict={} #Multimodal
                answer=running_msg

        else:
            answer='Sorry, the bot is offline -front-end dev in progress.'

        logging.info("[handle_bot_query]: Raw answer dict: "+str(answer_dict)[:2000]+'....')

        self.Mind.MIND_store_last_answer(answer,self.sesh_id)

        if answer_dict and False:
            ## Will send twice but recall on subsequent queries may require?
            logging.info("="*50)
            logging.info("[debug] auto broadcasting again cause not always propagates?")
            #** AUTO BROADCAST!
            ## Don't auto broadcast again..because done in interface_kb_ask for fresh data
            self.Mind.MIND_store_last_answer_meta(answer_dict,self.sesh_id)
        else:
            logging.info("[debug] NOT auto broadcasting again -- since was pre-central broadcast ws")
            pass
#            logging.info("[debug] STORING META POSSIBLY TWICE BUT SOMETIMES NOT 1")
            #logging.info("="*40)
            #logging.info("SKIP SENDING !!!    [debug] STORING META POSSIBLY TWICE BUT SOMETIMES NOT 1")
            #logging.info("="*40)
#           # self.Mind.MIND_store_last_answer_meta(answer_dict,self.sesh_id)

        if isinstance(answer,dict):
            logging.info("[warning] answer is dict, converting to json")
            if not answer:
                answer=''
            else:
                answer=json.dumps(answer)

        return answer,answer_dict
    
    def get_job_status(self,case_id):
        #> sim_user -> Smart_Agent job tracking
        # last run? is running? estimated time?
        #*recall, also use:
        
        # {'is_case_running_in_background': False, 'is_job_running': False, 'has_job_ever_finished': True, 'age_of_last_active_m': 2510.15, 'age_since_started_m': 2522.7591921448707, 'job_raw_page_count': 4, 'estimated_time_remaining': 0}

        is_background_job_running=False #[ ] TODO
        
        status=interface_get_job_status(case_id=case_id)

        has_job_ever_finished=status.get('has_job_ever_finished',False)
        is_running=False
        if not 'age_of_last_active_m' in status:
            msg='Case has not been processed. Please start processing.'
            return  is_running,has_job_ever_finished,msg


        msg=''

        if_not_seen_in_x_minutes=70 #then assume not running
        if status['age_of_last_active_m']<if_not_seen_in_x_minutes:
            ## Possibly
            if status['is_job_running']:
                is_running=True
                msg=''
                msg+='Case '+str(case_id)+' is being processed ('+str(int(status['age_since_started_m']))+' minutes).'
                msg+=' Possibly '+str(int(status['estimated_time_remaining']))+' minutes remaining.'
                is_running=True
            else:
                is_running=False

        else:
            is_running=False

        return is_running,has_job_ever_finished,msg


    
def dev1():
    ## STANDARD CASE:
    #- chat question is forwarded to interface_kb_ask to be answered
    #    - response is:  text answer
    #    - response is:  multimodal answer (dict) ie/ timeline data

    return

if __name__=='__main__':
    branches=['dev1']

    for b in branches:
        globals()[b]()
